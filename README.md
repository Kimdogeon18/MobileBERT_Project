# 모바일BERT 기반 가짜뉴스 분류 프로젝트

## 1. 프로젝트 개요
최근 인터넷과 SNS의 확산으로 인해 가짜뉴스가 빠르게 퍼지며 사회적 혼란과 여론 왜곡, 정치적 편향, 불안감 조성 등의 문제를 유발하고 있음. 이러한 상황 속에서, 사람의 수작업으로 정보를 판별하기에는 한계가 존재하며, 자동화된 뉴스 진위 판별 시스템의 수요가 커지고 있음.
이 프로젝트는 경량 사전학습 언어모델인 **MobileBERT**를 활용하여 뉴스 기사 본문을 기반으로 진위 여부를 분류하는 **이진 분류 모델**을 구축함.  

---

## 2. 프로젝트 목적 및 방향성
- 인터넷상에서 확산되는 가짜뉴스 문제를 자동화 기술로 해결하고자 함
- 사전학습된 경량 언어모델(MobileBERT)을 활용한 이진 분류 모델 개발
- 데이터 정제 전략, 라벨 밸런싱, 중복 제거 여부 등 다양한 데이터 설계 전략 실험
- 단순 모델 성능 비교를 넘어, 실전 상황에서의 모델 신뢰도 확보를 위한 기준 마련

---

## 3. 전체 데이터셋 구성 및 예시
- 총 샘플 수: 72,134건  
- 컬럼: `title`, `text`, `label`, `text_length`  
- 라벨 분포: 진짜 뉴스(1): 37,106건 / 가짜 뉴스(0): 35,028건  
- 텍스트 길이: 평균 약 3,270자 / 최대 142,961자

### 데이터 예시 (5건)

| title                          | text (일부)               | label | text_length |
|-------------------------------|---------------------------|--------|-------------|
| Trump meets with World Leaders | President Trump arrived… | 1      | 234         |
| BREAKING: Hillary Caught Lying | In a leaked email…        | 0      | 187         |
| NASA discovers water on Mars   | NASA announced that…      | 1      | 312         |
| Obama’s Secret Weapon Revealed | Sources claim that…       | 0      | 267         |
| Economic Growth at Record High | The latest data from…     | 1      | 294         |

---

## 4. 데이터 전처리 방식
- ✅ 100자 미만 본문 제거  
- ✅ 특수문자, HTML 태그, 공백 제거  
- ✅ 전체 소문자화 및 일반 불용어 제거  
- ✅ 중복 문장, 유사 텍스트 제거 (정제 강화 세트 기준)  
- ✅ 최대 토큰 길이 256 또는 512로 고정 
> 각 데이터셋은 실험 목적에 따라 정제 강도와 구성 전략을 달리함

---

## 5. 시각화 결과 및 분석

### 뉴스 본문 길이 분포 (진짜 vs 가짜)
- 진짜 뉴스는 평균적으로 더 긴 본문을 가짐 (특히 3000자 이하에서 큰 차이 발생)
- 모델 학습 시 입력 정보량 차이에 따른 학습 편향 가능성 존재  
![news_length_distribution_40000_limit](https://github.com/user-attachments/assets/e1a1b114-882e-43bd-b2bf-6317e134a527)


### 감성 점수 분포 비교 (진짜 vs 가짜)
- 진짜 뉴스: 중립 또는 긍정적 점수에 몰려 있음  
- 가짜 뉴스: 극단적으로 부정적이거나 긍정적인 감성 점수로 분포  
- 이는 가짜 뉴스가 자극적인 언어를 자주 사용하는 경향과 맞닿아 있음  
![sentiment_score_distribution](https://github.com/user-attachments/assets/173f8347-33c1-412e-8411-a2ff468c7263)


### 상위 키워드 비교 (TF-IDF 기준)
- 진짜 뉴스 키워드 예시: "president", "report", "agency", "statement", "research"
- 가짜 뉴스 키워드 예시: "breaking", "shocking", "secret", "proof", "fbi"
- 진짜 뉴스는 정보성 중심, 가짜 뉴스는 감성 자극 단어가 많았음  
![top_keywords_by_label](https://github.com/user-attachments/assets/ab320b4d-d7dc-4694-b2f6-806438520b78)


---

## 6. 대표 모델 성능 비교

| 모델명                 | 데이터셋                             | Train Acc | Val Acc | Inference Acc | ⚠ 과적합 여부 |
|------------------------|---------------------------------------|------------|----------|----------------|----------------|
| mobilebert_news_model  | Train_Sample_2500.csv                | 0.998      | 0.996    | 0.9972         | ✔ 없음         |
| mobilebert_stand_v1    | standardized_cleaned_WELFake_2000.csv | 0.985      | 0.945    | 0.9472         | ✔ 없음         |
| mobilebert_stand_v3    | standardized_cleaned_WELFake_2000_v3.csv | 0.986   | 0.954    | 0.9486         | ✔ 없음         |
| mobilebert_stand_v4    | standardized_cleaned_WELFake_2000_v4.csv | 0.980   | 0.916    | 0.8225         | ⚠ 있음         |
| mobilebert_balance_v1  | sampled_balanced_news_v1.csv         | 0.865      | 0.865    | 0.8487         | ✔ 없음         |

> 다양한 데이터 구성 전략과 전처리 방식에 따라 MobileBERT 성능이 어떻게 변동하는지 실험을 통해 정량적으로 분석함.  
> 정제 강도, 라벨 밸런싱, 중복 제거 여부 등은 추론 성능에 직접적인 영향을 미치는 핵심 요소로 작용하며, 이는 모델 개발 초기 단계에서의 데이터 설계 중요성을 시사함.

---

## 7. 성능 차이 분석 요약

- 동일한 MobileBERT 구조를 사용했음에도 불구하고 **데이터의 구성과 정제 전략**에 따라 모델의 성능이 크게 달라졌음  
- 예를 들어, 단순한 무작위 샘플링보다 라벨 밸런싱 및 중복 제거를 적용한 데이터에서 더 높은 일반화 성능이 확인됨  
- **중복 문장 제거**, **불용어 필터링**, **100자 미만 필터링** 등 전처리 기준을 강화할수록 모델의 예측 안정성 증가  
- 반면, 정제는 되었으나 토픽 다양성만 고려한 데이터의 경우, 훈련 정확도 대비 추론 성능 저하 발생 → 과적합 가능성 존재  
- 성능 차이는 모델이 **데이터 품질에 민감하게 반응**함을 실증적으로 보여주며, 이는 실제 모델 배포 환경에서도 중요한 고려 요소임

---

## 8. 주요 학습 데이터 구성

| 파일명                                 | 샘플 수 | 구성 방식                  | 정제 방식                   | 중복 제거 | 목적                             |
|----------------------------------------|---------|----------------------------|-----------------------------|-----------|----------------------------------|
| Train_Sample_2500.csv                  | 2500    | 무작위 추출                | 전처리 없음                  | 없음      | 초기 테스트용 베이스라인         |
| standardized_cleaned_WELFake_2000.csv | 2000    | 무작위 샘플링              | 기본 정제 (100자 필터 등)   | 없음      | 기본 비교용                      |
| standardized_cleaned_WELFake_2000_v3.csv | 2000 | 의미 기반 정제 + 중복 제거 | 불용어 및 유사 텍스트 제거 | 있음      | 정제 강도 테스트용               |
| standardized_cleaned_WELFake_2000_v4.csv | 2000 | 주제 다양성 기반 구성      | 중복 제거 + 토픽 조정       | 있음      | 일반화 성능 중심 실험           |
| sampled_balanced_news_v1.csv          | 4000    | 라벨 균형 유지 + 다양성 확보 | 토픽 기반 샘플링           | 있음      | 실사용 대비 검증용              |

---

## 9. 기술 스택
- Python 3.10
- PyTorch 2.x
- Hugging Face Transformers (MobileBERT)
- Scikit-learn, Pandas, NumPy
- Matplotlib, Seaborn, WordCloud
- KoNLPy, NLTK

---

## 10. 프로젝트 결론

### ✅ 결론  
- MobileBERT 구조 자체보다 **데이터 설계 및 정제 전략이 성능에 미치는 영향이 더 큼**  
- 모델 성능의 신뢰도는 전처리 방식, 중복 제거, 라벨 밸런싱 여부에 따라 크게 달라짐  
- 다양한 구성 실험을 통해 데이터 기반 분류 시스템의 실용성과 확장성을 검증함  

---

### 🔍 문제점 및 해결 방안  
- 초기 실험에서는 데이터 중복 및 불균형으로 과적합 발생  
→ 다양한 샘플링 전략 및 중복 제거를 통해 해결  
- 과한 정제는 정보 손실을 유발할 수 있음  
→ 정제 수준을 실험별로 조절하고 성능 비교로 최적화 유도

---

### 💡 배운 점  
- 경량화된 사전학습 모델도 충분히 높은 성능 가능  
- **데이터가 곧 성능**이라는 사실을 수치로 입증함  
- 실험 설계를 정교하게 할수록 모델 결과의 해석이 쉬워짐  
- 시각화와 분포 분석은 모델 입력 이해에 매우 효과적임

---
