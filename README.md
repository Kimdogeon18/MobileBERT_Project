# 모바일BERT 기반 가짜뉴스 분류 프로젝트

## 1. 프로젝트 개요
최근 인터넷과 SNS의 확산으로 인해 가짜뉴스가 빠르게 퍼지며 사회적 혼란과 여론 왜곡, 정치적 편향, 불안감 조성 등의 문제를 유발하고 있음. 이러한 상황 속에서, 사람의 수작업으로 정보를 판별하기에는 한계가 존재하며, 자동화된 뉴스 진위 판별 시스템의 수요가 커지고 있음.
이 프로젝트는 경량 사전학습 언어모델인 **MobileBERT**를 활용하여 뉴스 기사 본문을 기반으로 진위 여부를 분류하는 **이진 분류 모델**을 구축함.  

---

## 2. 프로젝트 목적 및 방향성
- 인터넷상에서 확산되는 가짜뉴스 문제를 자동화 기술로 해결하고자 함
- 사전학습된 경량 언어모델(MobileBERT)을 활용한 이진 분류 모델 개발
- 데이터 정제 전략, 라벨 밸런싱, 중복 제거 여부 등 다양한 데이터 설계 전략 실험
- 단순 모델 성능 비교를 넘어, 실전 상황에서의 모델 신뢰도 확보를 위한 기준 마련

---

## 3. 전체 데이터셋 구성 및 예시
- 총 샘플 수: 72,134건  
- 컬럼: `title`, `text`, `label`, `text_length`  
- 라벨 분포: 진짜 뉴스(1): 37,106건 / 가짜 뉴스(0): 35,028건  
- 텍스트 길이: 평균 약 3,270자 / 최대 142,961자

### 데이터 예시 (5건)

| title                          | text (일부)               | label | text_length |
|-------------------------------|---------------------------|--------|-------------|
| Trump meets with World Leaders | President Trump arrived… | 1      | 234         |
| BREAKING: Hillary Caught Lying | In a leaked email…        | 0      | 187         |
| NASA discovers water on Mars   | NASA announced that…      | 1      | 312         |
| Obama’s Secret Weapon Revealed | Sources claim that…       | 0      | 267         |
| Economic Growth at Record High | The latest data from…     | 1      | 294         |

---

## 4. 데이터 전처리 방식
- ✅ 100자 미만 본문 제거  
- ✅ 특수문자, HTML 태그, 공백 제거  
- ✅ 전체 소문자화 및 일반 불용어 제거  
- ✅ 중복 문장, 유사 텍스트 제거 (정제 강화 세트 기준)  
- ✅ 최대 토큰 길이 256 또는 512로 고정 
> 각 데이터셋은 실험 목적에 따라 정제 강도와 구성 전략을 달리함

---

## 5. 시각화 결과 및 분석

### 뉴스 본문 길이 분포 (진짜 vs 가짜)
- 진짜 뉴스는 평균적으로 더 긴 본문을 가짐 (특히 3000자 이하에서 큰 차이 발생)
- 모델 학습 시 입력 정보량 차이에 따른 학습 편향 가능성 존재  
![news_length_distribution_40000_limit](https://github.com/user-attachments/assets/e1a1b114-882e-43bd-b2bf-6317e134a527)


### 감성 점수 분포 비교 (진짜 vs 가짜)
- 진짜 뉴스: 중립 또는 긍정적 점수에 몰려 있음  
- 가짜 뉴스: 극단적으로 부정적이거나 긍정적인 감성 점수로 분포  
- 이는 가짜 뉴스가 자극적인 언어를 자주 사용하는 경향과 맞닿아 있음  
![sentiment_score_distribution](https://github.com/user-attachments/assets/173f8347-33c1-412e-8411-a2ff468c7263)


### 상위 키워드 비교 (TF-IDF 기준)
- 진짜 뉴스 키워드 예시: "president", "report", "agency", "statement", "research"
- 가짜 뉴스 키워드 예시: "breaking", "shocking", "secret", "proof", "fbi"
- 진짜 뉴스는 정보성 중심, 가짜 뉴스는 감성 자극 단어가 많았음  
![top_keywords_by_label](https://github.com/user-attachments/assets/ab320b4d-d7dc-4694-b2f6-806438520b78)


---

## 6. 대표 모델 성능 비교

| 모델명                 | 데이터셋                             | Train Acc | Val Acc | Inference Acc | ⚠ 과적합 여부 |
|------------------------|---------------------------------------|------------|----------|----------------|----------------|
| mobilebert_news_model  | Train_Sample_2500.csv                | 0.998      | 0.996    | 0.9972         | ✔ 없음         |
| mobilebert_stand_v1    | standardized_cleaned_WELFake_2000.csv | 0.985      | 0.945    | 0.9472         | ✔ 없음         |
| mobilebert_stand_v3    | standardized_cleaned_WELFake_2000_v3.csv | 0.986   | 0.954    | 0.9486         | ✔ 없음         |
| mobilebert_stand_v4    | standardized_cleaned_WELFake_2000_v4.csv | 0.980   | 0.916    | 0.8225         | ⚠ 있음         |
| mobilebert_balance_v1  | sampled_balanced_news_v1.csv         | 0.865      | 0.865    | 0.8487         | ✔ 없음         |

> 다양한 데이터 구성 전략과 전처리 방식에 따라 MobileBERT 성능이 어떻게 변동하는지 실험을 통해 정량적으로 분석함.  
> 정제 강도, 라벨 밸런싱, 중복 제거 여부 등은 추론 성능에 직접적인 영향을 미치는 핵심 요소로 작용하며, 이는 모델 개발 초기 단계에서의 데이터 설계 중요성을 시사함.

---

## 7. 성능 차이 분석 요약

- 동일한 MobileBERT 구조를 사용했음에도 불구하고 **데이터의 구성과 정제 전략**에 따라 모델의 성능이 크게 달라졌음  
- 예를 들어, 단순한 무작위 샘플링보다 라벨 밸런싱 및 중복 제거를 적용한 데이터에서 더 높은 일반화 성능이 확인됨  
- **중복 문장 제거**, **불용어 필터링**, **100자 미만 필터링** 등 전처리 기준을 강화할수록 모델의 예측 안정성 증가  
- 반면, 정제는 되었으나 토픽 다양성만 고려한 데이터의 경우, 훈련 정확도 대비 추론 성능 저하 발생 → 과적합 가능성 존재  
- 성능 차이는 모델이 **데이터 품질에 민감하게 반응**함을 실증적으로 보여주며, 이는 실제 모델 배포 환경에서도 중요한 고려 요소임

---

## 8. 주요 학습 데이터 구성

| 파일명                                 | 샘플 수 | 구성 방식                  | 정제 방식                   | 중복 제거 | 목적                             |
|----------------------------------------|---------|----------------------------|-----------------------------|-----------|----------------------------------|
| Train_Sample_2500.csv                  | 2500    | 무작위 추출                | 전처리 없음                  | 없음      | 초기 테스트용 베이스라인         |
| standardized_cleaned_WELFake_2000.csv | 2000    | 무작위 샘플링              | 기본 정제 (100자 필터 등)   | 없음      | 기본 비교용                      |
| standardized_cleaned_WELFake_2000_v3.csv | 2000 | 의미 기반 정제 + 중복 제거 | 불용어 및 유사 텍스트 제거 | 있음      | 정제 강도 테스트용               |
| standardized_cleaned_WELFake_2000_v4.csv | 2000 | 주제 다양성 기반 구성      | 중복 제거 + 토픽 조정       | 있음      | 일반화 성능 중심 실험           |
| sampled_balanced_news_v1.csv          | 4000    | 라벨 균형 유지 + 다양성 확보 | 토픽 기반 샘플링           | 있음      | 실사용 대비 검증용              |

---

## 9. 기술 스택
- Python 3.10
- PyTorch 2.x
- Hugging Face Transformers (MobileBERT)
- Scikit-learn, Pandas, NumPy
- Matplotlib, Seaborn, WordCloud
- KoNLPy, NLTK

---

## 10. 결론 및 회고

### ✅ 결론
- 이 프로젝트는 단순 분류 모델이 아닌, **데이터 품질 중심의 신뢰도 기반 분류 시스템 설계**를 목표로 한 실험적 시도임
- 모델 성능 개선의 핵심은 아키텍처 변경이 아닌 **데이터의 정제와 구성 전략**임을 다수의 실험을 통해 입증함
- 가짜뉴스 탐지와 같은 민감한 분야에서, 데이터 기반 설계는 모델 신뢰도를 좌우하는 결정적 요인이 됨

### 🧩 문제점 및 해결방안
- **과적합** 문제 발생: 일부 데이터셋에서 훈련 정확도는 높지만 추론 성능은 낮았음  
  → 라벨 불균형 해소, 중복 제거, 정제 강도 조절 등으로 개선
- **텍스트 길이 불균형**: 진짜 뉴스가 길이가 길어 학습 편향 유도  
  → 토큰 길이 제한, 길이 분포 분석 등을 통해 조정
- **의미 없는 학습 데이터** 존재  
  → 100자 미만 제거, 중복 텍스트 제거 등 사전 정제

### 💡 배운점
- 단순히 모델을 고도화하기보다는, **데이터 설계에 투자하는 것이 더 큰 성능 향상을 가져온다**는 점을 체감함
- 시각화, 정제, 라벨링 등 데이터를 바라보는 시각이 달라졌으며, 실전 투입 시 필요한 설계 감각을 얻음
- 사전학습 모델을 활용하더라도, 전처리 및 분포 분석 없이 적용하면 실전 성능과는 괴리가 발생함

---
