import torch
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import MobileBertForSequenceClassification, MobileBertTokenizer
from transformers import get_linear_schedule_with_warmup, logging
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from tqdm import tqdm
import chardet  # chardet ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ íŒŒì¼ ì¸ì½”ë”© ìë™ ê°ì§€

# 0. GPU í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# 1. ê²½ê³  ë©”ì‹œì§€ ì œê±°
logging.set_verbosity_error()

# 2. ë°ì´í„° ë¡œë“œ (ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©)
path = "cleaned_WELFake_2000.csv"

# ì¸ì½”ë”© ìë™ ê°ì§€í•˜ì—¬ CSV íŒŒì¼ ì½ê¸°
with open(path, 'rb') as f:
    result = chardet.detect(f.read())
    encoding = result['encoding']

# íŒŒì¼ì„ ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸°
df = pd.read_csv(path, encoding=encoding)

# ğŸ’¡ 'Headline' ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ ì‹¤ì œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ìˆ˜ì •
data_X = df['title'].astype(str).values  # 'title' ì»¬ëŸ¼ ì‚¬ìš© (ì´ë¦„ ë‹¤ë¥´ë©´ ìˆ˜ì •í•´ì•¼ í•¨)
labels = df['label'].values  # 'label' ì»¬ëŸ¼ ì‚¬ìš© (Fake=1, Real=0)

# 3. NaN ê°’ ì²˜ë¦¬
labels = labels.astype(float)  # labelsë¥¼ floatë¡œ ë³€í™˜í•˜ì—¬ NaN ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ í•¨
labels = np.nan_to_num(labels, nan=0)  # NaN ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´

# NaN ê°’ì´ ì—†ìœ¼ë©´ í™•ì¸ ë©”ì‹œì§€ ì¶œë ¥
if np.any(np.isnan(labels)):
    print("NaN ê°’ì´ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤!")

print(f"ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(data_X)}")
print("í—¤ë“œë¼ì¸ ì˜ˆì‹œ:", data_X[:3])
print("ë ˆì´ë¸” ì˜ˆì‹œ:", labels[:3])

# 4. í† í°í™”
tokenizer = MobileBertTokenizer.from_pretrained('mobilebert-uncased', do_lower_case=True)
inputs = tokenizer(list(data_X), truncation=True, max_length=64, padding="max_length", return_tensors="pt")

input_ids = inputs['input_ids']
attention_mask = inputs['attention_mask']

# 5. ë°ì´í„° ë¶„í• 
train_inputs, val_inputs, train_labels, val_labels = train_test_split(
    input_ids, labels, test_size=0.2, random_state=2025
)
train_masks, val_masks = train_test_split(attention_mask, test_size=0.2, random_state=2025)

# 6. í…ì„œ ë³€í™˜ (GPU/CPU ì´ë™)
# train_labelsì— ëŒ€í•´ NaN ê°’ì´ ì—†ë‹¤ê³  ê°€ì •í•˜ê³ , íƒ€ì…ì„ intë¡œ ë³€í™˜
train_inputs, train_labels, train_masks = train_inputs.to(device), torch.tensor(train_labels, dtype=torch.long).to(device), train_masks.to(device)
val_inputs, val_labels, val_masks = val_inputs.to(device), torch.tensor(val_labels, dtype=torch.long).to(device), val_masks.to(device)

train_data = TensorDataset(train_inputs, train_masks, train_labels)
val_data = TensorDataset(val_inputs, val_masks, val_labels)

train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=8)
val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=8)

# 7. ëª¨ë¸ ì„¤ì • (ê°€ì§œ ë‰´ìŠ¤ ì´ì§„ ë¶„ë¥˜)
model = MobileBertForSequenceClassification.from_pretrained('mobilebert-uncased', num_labels=2)
model.to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 4)

# 8. í•™ìŠµ ë£¨í”„
epochs = 5
epoch_results = []

for epoch in range(epochs):
    model.train()
    total_train_loss = 0
    total_train_correct = 0  # í•™ìŠµ ì •í™•ë„ ê³„ì‚°ì„ ìœ„í•œ ë³€ìˆ˜
    total_train_count = 0  # ì „ì²´ í•™ìŠµ ë°ì´í„° ìˆ˜

    progress_bar = tqdm(train_dataloader, desc=f"Training Epoch {epoch+1}")

    for batch in progress_bar:
        batch_ids, batch_mask, batch_labels = batch
        optimizer.zero_grad()

        output = model(batch_ids, attention_mask=batch_mask, labels=batch_labels)
        loss = output.loss
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        total_train_loss += loss.item()

        # í•™ìŠµ ì •í™•ë„ ê³„ì‚°
        preds = torch.argmax(output.logits, dim=1)
        total_train_correct += (preds == batch_labels).sum().item()
        total_train_count += batch_labels.size(0)

        progress_bar.set_postfix({'loss': loss.item()})

    avg_train_loss = total_train_loss / len(train_dataloader)
    train_accuracy = total_train_correct / total_train_count  # ì •í™•ë„ ê³„ì‚°

    # 9. ê²€ì¦ í‰ê°€
    model.eval()
    val_correct, val_total = 0, 0

    for batch in val_dataloader:
        batch_ids, batch_mask, batch_labels = batch
        with torch.no_grad():
            logits = model(batch_ids, attention_mask=batch_mask).logits
        preds = torch.argmax(logits, dim=1)
        val_correct += (preds == batch_labels).sum().item()
        val_total += batch_labels.size(0)

    val_accuracy = val_correct / val_total
    epoch_results.append((avg_train_loss, train_accuracy, val_accuracy))

    print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, Validation Accuracy = {val_accuracy:.4f}")

# 10. ëª¨ë¸ ì €ì¥
model.save_pretrained("mobilebert_fake_news_headline_detector1")
print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
